{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro To Keras!!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A little inspection and experiment with keras tells that\n",
    "----->It is high level Neural Netwoks API,written in Python runs on the top of Goggle's TensorFlow/Theano.\n",
    "As said by keras team -->\n",
    "            DEVELOPED WITH FOCUS ON ENABLING FAST EXPERIMENTATION.\n",
    "            Features include:--\n",
    "                ...User Friendliness\n",
    "                ...Modularity\n",
    "                ...Easy Extension\n",
    "                ...Pythonic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating The Keras Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Core datastructure of keras is a MODEL a way to organize layers.\n",
    "Model creation can be divided in 4 steps\n",
    "        1)  SPECIFY THE ARCHITECTURE\n",
    "        2)  COMPILE THE MODEL\n",
    "        3)  FIT THE MODEL TO THE DATA\n",
    "        4)  PREDICTION\n",
    "        \n",
    "Let's Inspect each one in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Ground(Model Architecture)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This step defines the \n",
    "1) number of layers we are going to use\n",
    "2) number of hidden unites we are going to use\n",
    "3) kind of activation function we are going to use\n",
    "4) input and output shape\n",
    "\n",
    "\n",
    "Let's build the model architecture--->>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Necessary Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load the dataset\n",
    "can = load_breast_cancer()\n",
    "df = pd.DataFrame(can.data)\n",
    "y = can.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets describe the data\n",
    "df.describe().T\n",
    "\n",
    "##From description it is obvious that we should normalize our dataset as normalization would affect the accuracy\n",
    "##of the model. We'll why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Standardizatioin \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = df.iloc[:,:].values\n",
    "X = sc.fit_transform(X)\n",
    "X\n",
    "\n",
    "##Thus our data is standardised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='non_gausian.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is the actual structure of our data and using gradient descent on this will lead us to local minima/pleatues which is not the optimization we wanted.\n",
    "Thus normalizing will lead to the range inputs to be more comparable thus the shape would be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='bell_3d.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Thus our optimization algorithm will surely find the minimum. So it will fasten the process of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's set up the architecture of the NN:---\n",
    "'''\n",
    "    A neural network consists of hidden layers having some number of units called as neurons and these neurons\n",
    "    functions as the computational site as all the computations occurs hare.\n",
    "    we will play around the neurons only.(Most of the time)\n",
    "'''\n",
    "\n",
    "##Importing the keras library\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "'''\n",
    "    DENSE:--all the nodes in the previous layers are connected with all nodes in the connected layer.\n",
    "        (Just like purely connected graph)(see image-1)\n",
    "\n",
    "    SEQUENTIAL:---Sequential model requires that each layers has weights or connections only to the one layer\n",
    "    comming directly after it in the network diagram.\n",
    "    \n",
    "    LAYERS:--- Keras layers are the fundamental building block of keras models. \n",
    "    \n",
    "    MODELS:---It is the core datastructure of the Keras\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename = \"dense.png\")\n",
    "##Representation of dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Deciding the input shape of the data\n",
    "print(\"shape of the df\",df.shape)\n",
    "print(len(np.unique(y)),'is the distinct outputs')\n",
    "##as we have 30 features so will be having 30 input nodes ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Configuration of our NN\n",
    "Input Shape = (30,) ## any number of rows with 30 input features\n",
    "OutPut shape = 2  ##only 2 neurons(Binary Classification)\n",
    "\n",
    "Number of layers --> 5\n",
    "H1(Hidden layer 1)---\n",
    "                    no of units(neurons):--Lets take 60 \n",
    "H2(Hidden layer 2)---\n",
    "                    no of units(neurons)---45\n",
    "H3(Hidden layer 3)---\n",
    "                    no of units(neurons)---30\n",
    "H3(Hidden layer 4)---\n",
    "                    no of units(neurons)---15\n",
    "Output layer (5th hidden layer) \n",
    "\n",
    "##Activation function in all the hidden layers will be \n",
    "ReLU(Rectified Linear Units)\n",
    "##Activation function in output would be \n",
    "Sigmoid(as it is binary classification model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lets make the model\n",
    "\n",
    "##Initializing the sequential class\n",
    "model = Sequential()\n",
    "\n",
    "## we stack the layers using ADD\n",
    "model.add(Dense(60,activation = 'relu',input_shape = (30,)))  ##H1 and input shape\n",
    "model.add(Dense(45,activation = 'relu'))                      ##H2 \n",
    "model.add(Dense(30,activation = 'relu'))                      ##H3\n",
    "model.add(Dense(15,activation = 'relu'))                      ##H4  \n",
    "model.add(Dense(2,activation = 'sigmoid'))                                           ##Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What we have did its just the bule print. No computation has started yet. \n",
    "Its time to supply an optimization function and loss function along with the metric of measurment.\n",
    "\n",
    "The Optimizer which we are going to use is gradientdescent the most basic one.\n",
    "loss function will be categorical_crossentropy as we are dealing with a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lets compile with compile\n",
    "import keras\n",
    "##Initialize the optimmizer\n",
    "opt = keras.optimizers.SGD(lr = 0.01)\n",
    "##SGD is short for Stochastic Gradient Descent where Batch size is 1\n",
    "##Learning rate is used to descend towards the minimum by optimizing the weights and biases\n",
    "##new_weight = old_weight-learning_rate*gradient\n",
    "\n",
    "##compilation with compile\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = opt,metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##As our input is binary so lets use to_categorical for the binary classification\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##fitting the model with .fit\n",
    "model.fit(X,y,epochs = 450)\n",
    "##epochs is no of loops we are going to run for the model optiimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Once we have trained the model its time for prediction.\n",
    "As we notice that we have started with 37% accuracy and at epoch no 375 we have reached 100%.\n",
    "We'll tune our model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets predict using predict\n",
    "pred = model.predict(X)\n",
    "##The prediction is a probability so we are going to take one axis and round it to see the accuracy\n",
    "pred = pred[:,[1]].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lets take the original targets\n",
    "y_true = y[:,[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets use confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true,pred)\n",
    "cm\n",
    "##Thus we have 19 false predictions \n",
    "##Taking about the accuracy its not good because that part is false negative and more dangerous.\n",
    "##That means 19 people were told that they dont had cancer but actually they had it.(so dangerous)\n",
    "##Now our target is to reduce false negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning The Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's BreakUp From Some neurons(DropOut)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data.\n",
    "It will randomly wipe out some neurons along with the connections with other neurons.\n",
    "we set a threashold value for that and it is b/w 0 to 1\n",
    "Lets see in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Importing dropout\n",
    "from keras.layers import Dropout\n",
    "\n",
    "##Initializing the sequential class\n",
    "new_model = Sequential()\n",
    "\n",
    "## we stack the layers using ADD\n",
    "new_model.add(Dense(60,activation = 'relu',input_shape = (30,)))  ##H1 and input shape\n",
    "##As we have more units se larger dropout\n",
    "new_model.add(Dropout(0.7))\n",
    "\n",
    "new_model.add(Dense(45,activation = 'relu'))                      ##H2 \n",
    "new_model.add(Dropout(0.6))\n",
    "\n",
    "new_model.add(Dense(30,activation = 'relu'))                      ##H3\n",
    "new_model.add(Dropout(0.5))\n",
    "\n",
    "##Dont want dropout hare\n",
    "new_model.add(Dense(15,activation = 'relu'))                      ##H4  \n",
    "\n",
    "new_model.add(Dense(2,activation = 'sigmoid')) \n",
    "##we must not have dropout for output layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the Optimizers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "we have a series of optimizers in Machine Learning like\n",
    "Gradient Descent with momentum\n",
    "Adam\n",
    "RmsProp\n",
    "\n",
    "We can try all but most of the times Adam outperforms all of these optimizers\n",
    "So Lets use Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Adam optimizer\n",
    "import keras\n",
    "new_opt = keras.optimizers.Adam(lr = 0.007)\n",
    "##rest all the parameters alpha and beta should be left default because they pretuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Compile the model\n",
    "new_model.compile(loss = 'categorical_crossentropy',optimizer = new_opt,metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets apply the Emergency Breaks:--"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "while fitting the model to the training data we can have many epochs but after some time accuracy do not improves thus we need to stop it early called as Early Stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fitting with the help of ealy stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience = 2)\n",
    "##patinece defines how many epochs our model can go without improving. i think 4 is great choice.\n",
    "new_model.fit(X,y,validation_split = 0.2,epochs = 1000,callbacks = [early_stopping_monitor])\n",
    "##validation split is used instead of k fold cross validation because k fold will take much of the training time\n",
    "##validation split gives better result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The point to note hare often early stopping dosent helps out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lets use newly trained model for prediction\n",
    "y_pred = new_model.predict(X)\n",
    "y_pred = y_pred[:,[1]].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y[:,[1]],y_pred)\n",
    "cm\n",
    "##Thus we see accuracy had increased it is 5 in false negative and we encountered 6 in false positive \n",
    "##Overall 11 which is better then 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Tuning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Thus far we have seen good accuracy\n",
    "But can we increase it further,Lets see\n",
    "One thing we can try is batch gradient descent and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "def gen_from_model():\n",
    "    ##Lets creat new model\n",
    "    model_2 = Sequential()\n",
    "\n",
    "    ## we stack the layers using ADD\n",
    "    model_2.add(Dense(60,activation = 'relu',input_shape = (30,)))  ##H1 and input shape\n",
    "    ##As we have more units se larger dropout\n",
    "    model_2.add(Dropout(0.7))\n",
    "\n",
    "    model_2.add(Dense(45,activation = 'relu'))                      ##H2 \n",
    "    model_2.add(Dropout(0.6))\n",
    "\n",
    "    model_2.add(Dense(30,activation = 'relu'))                      ##H3\n",
    "    model_2.add(Dropout(0.5))\n",
    "\n",
    "    ##Dont want dropout hare\n",
    "    model_2.add(Dense(15,activation = 'relu'))                      ##H4  \n",
    "\n",
    "    model_2.add(Dense(2,activation = 'sigmoid')) \n",
    "    ##we must not have dropout for output layers\n",
    "    return model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lets use train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "\n",
    "##Learn the learning rate\n",
    "learning_rate = [0.001,0.005,0.007,0.01]\n",
    "score = []\n",
    "for lr in learning_rate:\n",
    "    model_2 = gen_from_model()\n",
    "    opt = keras.optimizers.Adam(lr = lr)\n",
    "    ##compiling the model\n",
    "    model_2.compile(loss = 'categorical_crossentropy',optimizer = opt,metrics = ['accuracy'])\n",
    "    ##fitting the model\n",
    "    model_2.fit(X_train,y_train,epochs = 250)\n",
    "    ##Evaluating the score\n",
    "    scr = model_2.evaluate(X_test,y_test)\n",
    "    ##appending the result to score\n",
    "    score.append(scr)\n",
    "    \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Thus we see that learning_rate of 0.007 is best with low loss and high accuracy\n",
    "##One of the few things we can try batch greadient descent\n",
    "##Lets run a batch optimization with adam\n",
    "my_model = gen_from_model()\n",
    "opt = keras.optimizers.Adam(lr = 0.007)\n",
    "my_model.compile(loss = 'categorical_crossentropy',optimizer = opt,metrics = ['accuracy'])\n",
    "my_model.fit(X_train,y_train,epochs = 100,batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lets predict the results\n",
    "y_prd = my_model.predict(X_test)\n",
    "y_prd = y_prd[:,[1]].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test[:,[1]],y_prd)\n",
    "cm\n",
    "##This is improved result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run According To The Path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The name is rather funny, but we are going to have the same scenario.\n",
    "ReduceOnPlateau is keras function which reduced the learning rate by a factor when a metric has stopped improving.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets use reduce on Pleatue call back\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "call_model = gen_from_model()\n",
    "opt = keras.optimizers.Adam(lr = 0.007)\n",
    "call_model.compile(loss = 'categorical_crossentropy',optimizer = opt,metrics = ['accuracy'])\n",
    "pleatu_reduce = ReduceLROnPlateau(monitor = 'val_loss',factor = 0.2,patience = 5,min_lr = 0.0001)\n",
    "call_model.fit(X_train,y_train,epochs = 200,callbacks = [pleatu_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lets predict the results\n",
    "y_pd = call_model.predict(X_test)\n",
    "y_pd = y_pd[:,[1]].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test[:,[1]],y_pd)\n",
    "cm\n",
    "\n",
    "##This is more improved result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " According to me these are the challenges I face day to day while training a NN\n",
    " :--\n",
    " 1) The Dying Neuron Problem\n",
    "     --> Once a Node starts getting negative inputs ,it may continue only getting negative inputs. Thus it has nothing to help in modelling as a ReLU activation function will wipe this out.\n",
    "     Hence Dead Neuron\n",
    "     \n",
    "     Things we can try include:--\n",
    "      1>> Adjusting the weights \n",
    "      2>> Changing the activation function\n",
    "      3>> Using LeakyReLU instaed of ReLU.\n",
    "      4>> May be batch normalization will help.\n",
    " 2) Vanishing/Exploding Gradient\n",
    "        This occurs when many layers have small slopes,due to being on flat part of tanh curve.\n",
    "        Thus updates to backprop reduces to 0.\n",
    "        Hence the situation of dead Neuron arises.\n",
    "        \n",
    "        Things we can try include:--\n",
    "        Changing the activation function to ReLU\n",
    "        weight Regularization\n",
    "        \n",
    " 3) Other major problems include selection of hyperparameters and right choice of number of layers and units in each layers.\n",
    " This could be overcomed by Hit and Trial Method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things we can Try"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "we didnt tried few things which include:---\n",
    "    1) Batch Normalization:--\n",
    "        Its just like normalizing the inputs instead we normalize the outputs of each hidden units.\n",
    "        This will result in faster learning and we can use higher learning rate.\n",
    "    2) Regularzation techniques:--\n",
    "        well Adam takes care of it. But while using other optimization technique we should use it.\n",
    "        Recommended to use L2 norm.\n",
    "    3) Playing around different opotimization techniques.\n",
    "    \n",
    "\n",
    "\n",
    "Big ThankYou!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
